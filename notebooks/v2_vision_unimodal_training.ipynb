{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change present working directory to project directory\n",
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import pickle\n",
    "from sklearn.metrics import *\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from utils.utils import fix_the_random\n",
    "from models.vision_models import LSTM\n",
    "from utils.utils import load_config\n",
    "from training.evaluation import evalMetric\n",
    "from training.train_vision import train, validation\n",
    "from data_preprocessing.custom_datasets import Dataset_3DCNN, collate_fn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('configs/configs.yaml')\n",
    "fix_the_random(2021)\n",
    "\n",
    "ROOT_FOLDER = config[\"ROOT_FOLDER\"]\n",
    "DATASET_FOLDER = config[\"DATASET_FOLDER\"]\n",
    "\n",
    "# training parameters\n",
    "k = 2            # number of target category\n",
    "epochs = config[\"EPOCHS\"]\n",
    "batch_size = config[\"BATCH_SIZE\"]\n",
    "learning_rate = config[\"LEARNING_RATE\"]\n",
    "log_interval = config[\"LOG_INTERVAL\"]\n",
    "num_workers = config[\"NUM_WORKERS\"]\n",
    "pin_memory = config[\"PIN_MEMORY\"]\n",
    "\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU\n",
    "\n",
    "params = {\n",
    "    'batch_size': batch_size, \n",
    "    'shuffle': True, \n",
    "    'num_workers': num_workers, \n",
    "    'pin_memory': pin_memory} if use_cuda else {'batch_size': batch_size, 'shuffle': True}\n",
    "valParams = {\n",
    "    'batch_size': batch_size, \n",
    "    'shuffle': False, \n",
    "    'num_workers': num_workers, \n",
    "    'pin_memory': pin_memory} if use_cuda else {'batch_size': batch_size, 'shuffle': False}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load k-fold indexes\n",
    "with open(DATASET_FOLDER+'allFoldDetails.p', 'rb') as fp:\n",
    "    allDataAnnotation = pickle.load(fp)\n",
    "\n",
    "allF = ['fold1', 'fold2', 'fold3', 'fold4', 'fold5']\n",
    "\n",
    "finalOutputAccrossFold ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "\n",
      "Test set: (217 samples): Average loss: 0.5752, Accuracy: 71.43%, MF1 Score: 0.7075, F1 Score: 0.6630, Area Under Curve: 0.7134, Precision: 0.6224, Recall Score: 0.7093\n",
      "====================\n",
      "\n",
      "Test set: (109 samples): Average loss: 0.5472, Accuracy: 73.39%, MF1 Score: 0.7247, F1 Score: 0.6742, Area Under Curve: 0.7276, Precision: 0.6522, Recall Score: 0.6977\n",
      "====================\n",
      "\n",
      "Test set: (217 samples): Average loss: 0.5930, Accuracy: 70.97%, MF1 Score: 0.7040, F1 Score: 0.6631, Area Under Curve: 0.7116, Precision: 0.6139, Recall Score: 0.7209\n",
      "====================\n",
      "\n",
      "Test set: (109 samples): Average loss: 0.6026, Accuracy: 69.72%, MF1 Score: 0.6844, F1 Score: 0.6207, Area Under Curve: 0.6852, Precision: 0.6136, Recall Score: 0.6279\n",
      "====================\n",
      "\n",
      "Test set: (217 samples): Average loss: 0.7172, Accuracy: 69.59%, MF1 Score: 0.6834, F1 Score: 0.6207, Area Under Curve: 0.6842, Precision: 0.6136, Recall Score: 0.6279\n",
      "====================\n",
      "\n",
      "Test set: (109 samples): Average loss: 0.7335, Accuracy: 72.48%, MF1 Score: 0.7068, F1 Score: 0.6341, Area Under Curve: 0.7038, Precision: 0.6667, Recall Score: 0.6047\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# start training\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# train, test model\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     train_losses, train_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     test_loss, test_scores, veTest_pred \u001b[38;5;241m=\u001b[39m validation(comb, device, optimizer, test_loader)\n\u001b[1;32m     37\u001b[0m     test_loss1, test_scores1, veValid_pred \u001b[38;5;241m=\u001b[39m validation(comb, device, optimizer, valid_loader)\n",
      "File \u001b[0;32m~/Documents/Data_Science_Projects/Hate-Video-Classification/Hate_Video_Classification/CSE676-Deep-Learning-Final-Project/training/train_vision.py:33\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(log_interval, model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     30\u001b[0m N_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m X_text\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 33\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_text\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# output size = (batch, number of classes)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(output, y, weight\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mFloatTensor([\u001b[38;5;241m0.41\u001b[39m, \u001b[38;5;241m0.59\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#loss = F.cross_entropy(output, y)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Data_Science_Projects/Hate-Video-Classification/Hate_Video_Classification/CSE676-Deep-Learning-Final-Project/models/vision_models.py:12\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 12\u001b[0m     x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/rnn.py:878\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    875\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 878\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    882\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold in allF:\n",
    "    # train, test split\n",
    "    train_list, train_label= allDataAnnotation[fold]['train']\n",
    "    val_list, val_label  =  allDataAnnotation[fold]['val']\n",
    "    test_list, test_label  =  allDataAnnotation[fold]['test']\n",
    "\n",
    "\n",
    "    train_set, valid_set , test_set = Dataset_3DCNN(train_list, train_label), Dataset_3DCNN(val_list, val_label), Dataset_3DCNN(test_list, test_label)\n",
    "    train_loader = data.DataLoader(train_set, collate_fn = collate_fn, **params)\n",
    "    test_loader = data.DataLoader(test_set, collate_fn = collate_fn, **valParams)\n",
    "    valid_loader = data.DataLoader(valid_set, collate_fn = collate_fn, **valParams)\n",
    "\n",
    "    comb = LSTM().to(device)\n",
    "\n",
    "    # Parallelize model to multiple GPUs\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        comb = nn.DataParallel(comb)\n",
    "\n",
    "    optimizer = torch.optim.Adam(comb.parameters(), lr=learning_rate)   # optimize all cnn parameters\n",
    "\n",
    "    epoch_train_losses = []\n",
    "    epoch_train_scores = []\n",
    "    epoch_test_losses = []\n",
    "    epoch_test_scores = []\n",
    "\n",
    "    validFinalValue = None\n",
    "    testFinalValue = None\n",
    "    finalScoreAcc =0\n",
    "    prediction  = None\n",
    "\n",
    "    # start training\n",
    "    for epoch in range(epochs):\n",
    "        # train, test model\n",
    "        train_losses, train_scores = train(log_interval, comb, device, train_loader, optimizer, epoch)\n",
    "        test_loss, test_scores, veTest_pred = validation(comb, device, optimizer, test_loader)\n",
    "        test_loss1, test_scores1, veValid_pred = validation(comb, device, optimizer, valid_loader)\n",
    "        if (test_scores1['mF1Score']>finalScoreAcc):\n",
    "            finalScoreAcc = test_scores1['mF1Score']\n",
    "            validFinalValue = test_scores1\n",
    "            testFinalValue = test_scores\n",
    "            prediction = {'test_list': test_list , 'test_label': test_label, 'test_pred': veTest_pred}\n",
    "\n",
    "        # save results\n",
    "        epoch_train_losses.append(train_losses)\n",
    "        epoch_train_scores.append(list(x['accuracy'] for x in train_scores))\n",
    "        epoch_test_losses.append(test_loss)\n",
    "        epoch_test_scores.append(test_scores['accuracy'])\n",
    "\n",
    "\n",
    "        # save all train test results\n",
    "        A = np.array(epoch_train_losses)\n",
    "        B = np.array(epoch_train_scores)\n",
    "        C = np.array(epoch_test_losses)\n",
    "        D = np.array(epoch_test_scores)\n",
    "    finalOutputAccrossFold[fold] = {'validation':validFinalValue, 'test': testFinalValue, 'test_prediction': prediction}\n",
    "        \n",
    "\n",
    "with open('foldWiseRes_lstmVision.p', 'wb') as fp:\n",
    "    pickle.dump(finalOutputAccrossFold,fp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# allValueDict ={}\n",
    "# for fold in allF:\n",
    "#     for val in finalOutputAccrossFold[fold]['test']:\n",
    "#         try:\n",
    "#             allValueDict[val].append(finalOutputAccrossFold[fold]['test'][val])\n",
    "#         except:\n",
    "#             allValueDict[val]=[finalOutputAccrossFold[fold]['test'][val]]\n",
    "\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# for i in allValueDict:\n",
    "#     print(f\"{i} : Mean {np.mean(allValueDict[i])}  STD: {np.std(allValueDict[i])}\")\n",
    "\n",
    "\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
